{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Concept GNN Pipeline – Colab GPU A100\n",
    "\n",
    "Ce notebook prépare et exécute l'ensemble du pipeline (génération des données + entraînement GNN) sur un GPU A100. Chaque étape est isolée dans une cellule afin de pouvoir valider progressivement le processus avant les runs massifs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prerequisites"
   },
   "source": [
    "## 1. Pré-requis\n",
    "- Type d'instance : **GPU A100** (Runtime Colab → *Changer le type d'exécution* → GPU → A100).\n",
    "- Dépôt GitHub organisé selon la structure `concept-gnn-pipeline`.\n",
    "- Fichier `requirements.txt` à la racine du dépôt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu",
    "executionInfo": {}
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install"
   },
   "source": [
    "## 2. Installation des dépendances\n",
    "- Cloner le dépôt\n",
    "- Installer les dépendances (PyTorch GPU, torch_geometric, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "env_setup"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ⬇️ Renseigner l'URL du dépôt (public ou via token HTTPS)\n",
    "REPO_URL = \"https://github.com/<org>/concept-gnn-pipeline.git\"\n",
    "REPO_DIR = \"/content/concept-gnn-pipeline\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone --depth 1 {REPO_URL} {REPO_DIR}\n",
    "else:\n",
    "    %cd {REPO_DIR}\n",
    "    !git pull\n",
    "\n",
    "%cd {REPO_DIR}\n",
    "\n",
    "# Installation des dépendances\n",
    "!pip install -U pip\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "configs"
   },
   "source": [
    "## 3. Vérification / édition des fichiers de configuration\n",
    "\n",
    "Adapter les différents YAML/JSON si nécessaire avant de lancer les scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show_config"
   },
   "outputs": [],
   "source": [
    "!ls config\n",
    "\n",
    "# Exemple : afficher bins.yml\n",
    "!cat config/bins.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sweep"
   },
   "source": [
    "## 4. Balayage métrique déterministe\n",
    "Génère `analysis/effect_metric_sweep.csv` pour inspecter l’effet de chaque chaîne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_sweep"
   },
   "outputs": [],
   "source": [
    "!python scripts/sweep_metrics.py --config config/bins.yml --output analysis/effect_metric_sweep.csv\n",
    "!head -n 5 analysis/effect_metric_sweep.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coverage_gen"
   },
   "source": [
    "## 5. Génération de dataset avec couverture contrôlée\n",
    "Paramètres définis dans `config/generator.yml`. Ajuster `--max-samples`, `--coverage-threshold`, `--min-per-bin` selon la taille voulue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_dataset"
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = \"analysis/train_dataset.jsonl\"\n",
    "\n",
    "!python scripts/generate_dataset.py \\\n",
    "    --config config/generator.yml \\\n",
    "    --max-samples 20000 \\\n",
    "    --coverage-threshold 0.7 \\\n",
    "    --min-per-bin 20 \\\n",
    "    --output {DATASET_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inspect_dataset"
   },
   "source": [
    "## 6. Inspection rapide du dataset\n",
    "- Vérifier quelques enregistrements\n",
    "- Visualiser la couverture des bins et la distribution des métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset_stats"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Lecture partielle du JSONL\n",
    "records = []\n",
    "with open(DATASET_PATH) as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx >= 5000:  # limiter l'aperçu\n",
    "            break\n",
    "        rec = json.loads(line)\n",
    "        metrics = rec[\"metrics\"]\n",
    "        metrics[\"sequence_id\"] = rec[\"sequence_id\"]\n",
    "        records.append(metrics)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "coverage_plot"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(df['fft_anisotropy'], bins=30)\n",
    "plt.title('Distribution de l\\'anisotropie FFT')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(df['mutual_info_local'], bins=30)\n",
    "plt.title('Distribution de la MI locale')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train"
   },
   "source": [
    "## 7. Entraînement GNN (démarrage) \n",
    "Adapter les hyperparamètres dans `config/train.yml`. Cette cellule lance un training de base, sauvegarde les checkpoints, et écrit les métriques epoch par epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_script"
   },
   "outputs": [],
   "source": [
    "LOG_DIR = \"analysis/run_colab\"\n",
    "!python scripts/train_gnn.py \\\n",
    "    --config config/train.yml \\\n",
    "    --dataset {DATASET_PATH} \\\n",
    "    --log-dir {LOG_DIR} \\\n",
    "    --epochs 20 \\\n",
    "    --checkpoint-every 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monitor"
   },
   "source": [
    "## 8. Analyse des métriques d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_metrics"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "records = []\n",
    "with open(f\"{LOG_DIR}/metrics_log.jsonl\") as f:\n",
    "    for line in f:\n",
    "        records.append(json.loads(line))\n",
    "\n",
    "epochs = [r['epoch'] for r in records]\n",
    "train_loss = [r['train_loss'] for r in records]\n",
    "train_acc = [r['train_acc'] for r in records]\n",
    "val_acc = [r['val_acc'] for r in records]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(7,4))\n",
    "ax1.plot(epochs, train_loss, label='Train Loss', color='tab:red')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color='tab:red')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs, train_acc, label='Train Acc', color='tab:blue')\n",
    "ax2.plot(epochs, val_acc, label='Val Acc', color='tab:green')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=3)\n",
    "plt.title('Suivi entraînement GNN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluate"
   },
   "source": [
    "## 9. Évaluation & Inversion\n",
    "En utilisant le checkpoint de votre choix, lancez une évaluation complète ou un test d'inversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_script"
   },
   "outputs": [],
   "source": [
    "!python scripts/evaluate_gnn.py \\\n",
    "    --checkpoint {LOG_DIR}/ckpt_epoch20.pt \\\n",
    "    --dataset analysis/test_dataset.jsonl \\\n",
    "    --topk 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "artifacts"
   },
   "source": [
    "## 10. Sauvegarde / export des artifacts\n",
    "Compresser logs + checkpoints, et les télécharger via Colab ou les envoyer sur Drive/Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "archive"
   },
   "outputs": [],
   "source": [
    "!tar -czf run_colab_artifacts.tar.gz {LOG_DIR} analysis/effect_metric_sweep.csv\n",
    "from google.colab import files\n",
    "files.download('run_colab_artifacts.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## 11. Étapes suivantes\n",
    "- Ajuster la génération (augmenter `max-samples`, affiner les bins).\n",
    "- Tester différentes architectures (Graph Transformer + pos enc, attention multi-échelle).\n",
    "- Lancer l’entraînement en streaming Distribué (multi-GPU) sur cluster GH200/H100.\n",
    "- Intégrer des pipelines d’inversion et d’estimation WebP dans un service temps réel.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "concept_gnn_pipeline.ipynb",
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
